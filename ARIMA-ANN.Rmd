---
title: "Projet_recherche"
output: html_document
date: "2024-03-15"
---

# Installing the necessary package 
```{r setup, include=FALSE}
set.seed(123)
options(repos = c(CRAN = "https://cloud.r-project.org"))
library(forecast)
library(caret)
install.packages("tfruns")
library(tfruns)
library(tensorflow)
library(keras)
library(tfdatasets)
library(fma)

Sys.setenv(TENSORFLOW_DEFAULT_GPU_DEVICE = "gpu")


tensorflow::set_random_seed(123) #For the reproductibility of our ANN 
```

# Upload the same data as the paper 
```{r}

data_lynx<-log10(lynx)
data(ibmclose)
data_ibm<-ibmclose
data_sunspot<-ts(sunspot.year[1:288])

plot.ts(data_lynx)
plot.ts(data_ibm)
plot.ts(data_sunspot)

```

# Split into train_data and test_data
```{r}
train_d_lynx<-data_lynx[1:100]
test_d_lynx<-data_lynx[101:114]

train_d_ibm<-data_ibm[1:299]
test_d_ibm<-data_ibm[300:369]

train_d_sunspot<-data_sunspot[1:221]
test_d_sunspot<-data_sunspot[222:288]

```

## Start with ARIMA models

```{r}
### ARIMA Lynx
# Train_data
ar_lynx<-arima(train_d_lynx,order=c(11,0,0))
predictions_arima_lynx<-fitted(ar_lynx)
mse_arima_train_lynx<-mean((predictions_arima_lynx[8:100]-train_d_lynx[8:100])^2) #For the comparaison with ANN models who take 7 inputs
plot.ts(train_d_lynx)
lines(predictions_arima_lynx,lty=2,col='red')

# Test_data
pred_lynx<-predict(ar_lynx,n.ahead=length(test_d_lynx))
mse_arima_test_lynx<-mean((test_d_lynx-pred_lynx$pred)^2)
plot.ts(test_d_lynx,ylim = range(c(test_d_lynx, data.frame(pred_lynx$pred))))
lines(data.frame(pred_lynx$pred),lty=2,col='red',)

```

```{r}
### ARIMA IBM
# Train_data
ar_ibm<-arima(train_d_ibm,order=c(1,1,2))
predictions_arima_ibm<-fitted(ar_ibm)
mse_arima_train_ibm<-mean((predictions_arima_ibm[7:299]-train_d_ibm[7:299])^2) #For the comparaison with ANN models who take 6 inputs
plot.ts(train_d_ibm)
lines(predictions_arima_ibm,lty=2,col='red')

# Test_data
pred_ibm<-predict(ar_ibm,n.ahead=length(test_d_ibm))
mse_arima_test_ibm<-mean((test_d_ibm-pred_ibm$pred)^2)
plot.ts(test_d_ibm,ylim = range(c(test_d_ibm, data.frame(pred_ibm$pred))))
lines(data.frame(pred_ibm$pred),lty=2,col='red')
```

```{r}
### ARIMA Sunspot
# Train_data
ar_sunspot<-arima(train_d_sunspot,order=c(9,0,0))
predictions_arima_sunspot<-fitted(ar_sunspot)
mse_arima_train_sunspot<-mean((predictions_arima_sunspot[5:100]-train_d_sunspot[5:100])^2) #For the comparaison with ANN models who take 4 inputs
plot.ts(train_d_sunspot)
lines(predictions_arima_sunspot,lty=2,col='red')

# Test_data
pred_sunspot<-predict(ar_sunspot,n.ahead=length(test_d_sunspot))
mse_arima_test_sunspot<-mean((test_d_sunspot-pred_sunspot$pred)^2)
plot.ts(test_d_sunspot,ylim = range(c(test_d_sunspot, data.frame(pred_sunspot$pred))))
lines(data.frame(pred_sunspot$pred),lty=2,col='red',)
```

### Now ANN model alone

Make function to normalize and renormalize for the ann models
```{r, include=FALSE}
# We need to normalize our data before use them in ANN 

min_max_scale <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

renormaliser <- function(x, min_val, max_val) {
  denormalized <- x * (max_val - min_val) + min_val
  return(denormalized)
  }
```

## Lynx data

# Train our model
```{r,include=FALSE}
tensorflow::set_random_seed(123) #For the reproductibility
train <- data.frame(train_d_lynx)
# Define the number of folds for cross-validation
num_folds <- 10
set.seed(123)
# Create a data partition for cross-validation
index <-lapply(1:num_folds, function(i) ((i - 1) * 10 + 1):(i * 10))

create_model <- function(input_nodes,hidden_nodes) {
  set.seed(123)
  inputs <- layer_input(shape = c(input_nodes, 1))
  outputs <- inputs %>%
    layer_flatten()%>%
    layer_dense(hidden_nodes,activation ="tanh") %>%
    layer_dense(1, activation='linear')
  
  model <- keras_model(inputs, outputs)
  
  model %>% compile(
    optimizer = "rmsprop",
    loss = "mse",
    metrics="mae"
  )
  return(model)
}

best_mse<-100000
cv_results1<-numeric(num_folds)

train_model <- function(input_nodes, hidden_nodes) {
  for (t in 1:10 ){
    train_sequence <- train[-index[[t]], ]
    val_sequence <- train[index[[t]], ]
    
    train_dataset <- timeseries_dataset_from_array(
    data =  min_max_scale(train_sequence),
    targets = tail(min_max_scale(train_sequence),-input_nodes),
    sequence_length = input_nodes,
  )
  
    val_dataset <- timeseries_dataset_from_array(
    data = min_max_scale(val_sequence),
    targets = tail(min_max_scale(val_sequence),-input_nodes),
    sequence_length = input_nodes,
  )
  
    
    # Create and train the model
    model <- create_model(input_nodes, hidden_nodes)
    model %>% fit(
      train_dataset,
      epochs = 1000,
      shuffle=FALSE,
    )
  
    # Evaluate the model on the test set
    predictions <- model %>% predict(val_dataset)
    pred_final<-renormaliser(predictions,min(val_sequence), max(val_sequence))
    mse <- mean((pred_final - tail(val_sequence,-input_nodes))^2)
    cv_results1[t]<-mse
    if (mse<best_mse){
      best_model <- model
      best_mse<-mse}
  }
  
  return(c(MSE = mean(cv_results1),Mdl=best_model))
}

input_nodes<- 7
hidden_nodes<- 5 

results_train_lynx_ann<-train_model(input_nodes,hidden_nodes)
best_model_ann_lynx<-results_train_lynx_ann$Mdl
mse_ann_train_lynx<- results_train_lynx_ann$MSE

complete_train_lynx<-timeseries_dataset_from_array(
    data =  min_max_scale(train_d_lynx),
    targets = tail(min_max_scale(train_d_lynx),-7),
    sequence_length = 7,
  )
pred_complete_train_ann_lynx<-best_model_ann_lynx%>%predict(complete_train_lynx)
pred_complete_train_ann_lynx<-renormaliser(pred_complete_train_ann_lynx,min(train),max(train))
mse_complete_train_ann_lynx<-mean((tail(train_d_lynx,-7)-pred_complete_train_ann_lynx)^2)
```

## Test data

```{r}
test<-data_lynx[94:114]
test_dataset <- timeseries_dataset_from_array(
  data = min_max_scale(test),
  targets = min_max_scale(test)[101:114],
  sequence_length = 7,
)
test_ann_lynx<-best_model_ann_lynx%>%predict(test_dataset)
test_ann_lynx<-renormaliser(test_ann_lynx,min(test),max(test))
mse_ann_test_lynx<-mean((test_d_lynx-test_ann_lynx)^2)
```

## IBM data

# Train our model
```{r,include=FALSE}
tensorflow::set_random_seed(123) #For the reproductibility
train <- data.frame(train_d_ibm)
# Define the number of folds for cross-validation
num_folds <- 10
set.seed(123)
# Create a data partition for cross-validation
index <-lapply(1:num_folds, function(i) ((i - 1) * 10 + 1):(i * 10))

create_model <- function(input_nodes,hidden_nodes) {
  set.seed(123)
  inputs <- layer_input(shape = c(input_nodes, 1))
  outputs <- inputs %>%
    layer_flatten()%>%
    layer_dense(hidden_nodes,activation ="tanh") %>%
    layer_dense(1, activation='linear')
  
  model <- keras_model(inputs, outputs)
  
  model %>% compile(
    optimizer = "rmsprop",
    loss = "mse",
    metrics="mae"
  )
  return(model)
}

best_mse<-100000
cv_results1<-numeric(num_folds)

train_model <- function(input_nodes, hidden_nodes) {
  for (t in 1:10 ){
    train_sequence <- train[-index[[t]], ]
    val_sequence <- train[index[[t]], ]
    
    train_dataset <- timeseries_dataset_from_array(
    data =  min_max_scale(train_sequence),
    targets = tail(min_max_scale(train_sequence),-input_nodes),
    sequence_length = input_nodes,
  )
  
    val_dataset <- timeseries_dataset_from_array(
    data = min_max_scale(val_sequence),
    targets = tail(min_max_scale(val_sequence),-input_nodes),
    sequence_length = input_nodes,
  )
  
    
    # Create and train the model
    model <- create_model(input_nodes, hidden_nodes)
    model %>% fit(
      train_dataset,
      epochs = 1000,
      shuffle=FALSE,
    )
  
    # Evaluate the model on the test set
    predictions <- model %>% predict(val_dataset)
    pred_final<-renormaliser(predictions,min(val_sequence), max(val_sequence))
    mse <- mean((pred_final - tail(val_sequence,-input_nodes))^2)
    cv_results1[t]<-mse
    if (mse<best_mse){
      best_model <- model
      best_mse<-mse}
  }
  
  return(c(MSE = mean(cv_results1),Mdl=best_model))
}

input_nodes<- 6
hidden_nodes<- 5 

results_train_ibm_ann<-train_model(input_nodes,hidden_nodes)
best_model_ann_ibm<-results_train_ibm_ann$Mdl
mse_ann_train_ibm<- results_train_ibm_ann$MSE

complete_train_ibm<-timeseries_dataset_from_array(
    data =  min_max_scale(train_d_ibm),
    targets = tail(min_max_scale(train_d_ibm),-6),
    sequence_length = 6,
  )
pred_complete_train_ann_ibm<-best_model_ann_ibm%>%predict(complete_train_ibm)
pred_complete_train_ann_ibm<-renormaliser(pred_complete_train_ann_ibm,min(train),max(train))
mse_complete_train_ann_ibm<-mean((tail(train_d_ibm,-6)-pred_complete_train_ann_ibm)^2)
```

## Test data

```{r}
test<-data_ibm[294:369]
test_dataset <- timeseries_dataset_from_array(
  data = min_max_scale(test),
  targets = min_max_scale(test)[300:369],
  sequence_length = 6,
)
test_ann_ibm<-best_model_ann_ibm%>%predict(test_dataset)
test_ann_ibm<-renormaliser(test_ann_ibm,min(test),max(test))
mse_ann_test_ibm<-mean((test_d_ibm-test_ann_ibm)^2)
```

## Sunspot data

# Train our model
```{r,include=FALSE}
tensorflow::set_random_seed(123) #For the reproductibility
train <- data.frame(train_d_sunspot)
# Define the number of folds for cross-validation
num_folds <- 10
set.seed(123)
# Create a data partition for cross-validation
index <-lapply(1:num_folds, function(i) ((i - 1) * 10 + 1):(i * 10))

create_model <- function(input_nodes,hidden_nodes) {
  set.seed(123)
  inputs <- layer_input(shape = c(input_nodes, 1))
  outputs <- inputs %>%
    layer_flatten()%>%
    layer_dense(hidden_nodes,activation ="tanh") %>%
    layer_dense(1, activation='linear')
  
  model <- keras_model(inputs, outputs)
  
  model %>% compile(
    optimizer = "rmsprop",
    loss = "mse",
    metrics="mae"
  )
  return(model)
}

best_mse<-100000
cv_results1<-numeric(num_folds)

train_model <- function(input_nodes, hidden_nodes) {
  for (t in 1:10 ){
    train_sequence <- train[-index[[t]], ]
    val_sequence <- train[index[[t]], ]
    
    train_dataset <- timeseries_dataset_from_array(
    data =  min_max_scale(train_sequence),
    targets = tail(min_max_scale(train_sequence),-input_nodes),
    sequence_length = input_nodes,
  )
  
    val_dataset <- timeseries_dataset_from_array(
    data = min_max_scale(val_sequence),
    targets = tail(min_max_scale(val_sequence),-input_nodes),
    sequence_length = input_nodes,
  )
  
    
    # Create and train the model
    model <- create_model(input_nodes, hidden_nodes)
    model %>% fit(
      train_dataset,
      epochs = 1000,
      shuffle=FALSE,
    )
  
    # Evaluate the model on the test set
    predictions <- model %>% predict(val_dataset)
    pred_final<-renormaliser(predictions,min(val_sequence), max(val_sequence))
    mse <- mean((pred_final - tail(val_sequence,-input_nodes))^2)
    cv_results1[t]<-mse
    if (mse<best_mse){
      best_model <- model
      best_mse<-mse}
  }
  
  return(c(MSE = mean(cv_results1),Mdl=best_model))
}

input_nodes<- 4
hidden_nodes<- 4

results_train_sunspot_ann<-train_model(input_nodes,hidden_nodes)
best_model_ann_sunspot<-results_train_sunspot_ann$Mdl
mse_ann_ann_sunspot<- results_train_sunspot_ann$MSE

complete_train_sunspot<-timeseries_dataset_from_array(
    data =  min_max_scale(train_d_sunspot),
    targets = tail(min_max_scale(train_d_sunspot),-4),
    sequence_length = 4,
  )
pred_complete_train_ann_sunspot<-best_model_ann_sunspot%>%predict(complete_train_sunspot)
pred_complete_train_ann_sunspot<-renormaliser(pred_complete_train_ann_sunspot,min(train),max(train))
mse_complete_train_ann_sunspot<-mean((tail(train_d_sunspot,-4)-pred_complete_train_ann_sunspot)^2)
```

## Test data

```{r}
test<-data_sunspot[218:288]
test_dataset <- timeseries_dataset_from_array(
  data = min_max_scale(test),
  targets = min_max_scale(test)[222:288],
  sequence_length = 4,
)
test_ann_sunspot<-best_model_ann_sunspot%>%predict(test_dataset)
test_ann_sunspot<-renormaliser(test_ann_sunspot,min(test),max(test))
mse_ann_test_sunspot<-mean((test_d_sunspot-test_ann_sunspot)^2)
```

#### Second step : create the ANN-ARIMA model 

We have two type of model, the additive model and multiplicative model.
We have already use our arima model so now we can create our ANN model for our ANN-ARIMA model
```{r}
# Lynx data

add_train_lynx = train_d_lynx-predictions_arima_lynx
multi_train_lynx= train_d_lynx/predictions_arima_lynx

# IBM data 

add_train_ibm = train_d_ibm-predictions_arima_ibm
multi_train_ibm= train_d_ibm/predictions_arima_ibm

# Sunspot data 

add_train_sunspot = train_d_sunspot-predictions_arima_sunspot
multi_train_sunspot= train_d_sunspot/predictions_arima_sunspot

```

We need to normalize our data before use them in ANN 

```{r, include=FALSE}

# Lynx data 

normalized_add_train_lynx <- data.frame(min_max_scale(add_train_lynx))
normalized_multi_train_lynx <- data.frame(min_max_scale(multi_train_lynx))

# IBM data 

normalized_add_train_ibm <- data.frame(min_max_scale(add_train_ibm))
normalized_multi_train_ibm <- data.frame(min_max_scale(multi_train_ibm))

# Sunspot data 

normalized_add_train_sunspot <- data.frame(min_max_scale(add_train_sunspot))
normalized_multi_train_sunspot <- data.frame(min_max_scale(multi_train_sunspot))

```

### Additive model first 

## Lynx data 

# Train our model 

```{r,include=FALSE}
tensorflow::set_random_seed(123) #For the reproductibility
train <- normalized_add_train_lynx
# Define the number of folds for cross-validation
num_folds <- 10
set.seed(123)
# Create a data partition for cross-validation
index <-lapply(1:num_folds, function(i) ((i - 1) * 10 + 1):(i * 10))

create_model <- function(input_nodes,hidden_nodes) {
  set.seed(123)
  inputs <- layer_input(shape = c(input_nodes, 1))
  outputs <- inputs %>%
    layer_flatten()%>%
    layer_dense(hidden_nodes,activation ="tanh") %>%
    layer_dense(1, activation='linear')
  
  model <- keras_model(inputs, outputs)
  
  model %>% compile(
    optimizer = "rmsprop",
    loss = "mse",
    metrics="mae"
  )
  return(model)
}

best_mse<-100000
cv_results1<-numeric(num_folds)

train_model <- function(input_nodes, hidden_nodes) {
  for (t in 1:10 ){
    train_sequence <- train[-index[[t]], ]
    val_sequence <- train[index[[t]], ]
    
    train_dataset <- timeseries_dataset_from_array(
    data =  train_sequence,
    targets = tail(train_sequence,-input_nodes),
    sequence_length = input_nodes,
  )
  
    val_dataset <- timeseries_dataset_from_array(
    data = val_sequence,
    targets = tail(val_sequence,-input_nodes),
    sequence_length = input_nodes,
  )
  
    
    # Create and train the model
    model <- create_model(input_nodes, hidden_nodes)
    model %>% fit(
      train_dataset,
      epochs = 1000,
      shuffle=FALSE,
    )
  
    # Evaluate the model on the test set
    predictions <- model %>% predict(val_dataset)
    pred_final<-renormaliser(predictions,min(val_sequence), max(val_sequence))
    mse <- mean((pred_final - tail(val_sequence,-input_nodes))^2)
    cv_results1[t]<-mse
    if (mse<best_mse){
      best_model <- model
      best_mse<-mse}
  }
  
  return(c(MSE = mean(cv_results1),Mdl=best_model))
}

input_nodes<- 7
hidden_nodes<- 5 

results_train_lynx_add<-train_model(input_nodes,hidden_nodes)
best_model_add_lynx<-results_train_lynx_add$Mdl
mse_add_train_lynx<- results_train_lynx_add$MSE

complete_train_lynx<-timeseries_dataset_from_array(
    data =  min_max_scale(add_train_lynx),
    targets = tail(min_max_scale(add_train_lynx),-7),
    sequence_length = 7,
  )
pred_complete_train_add_lynx<-best_model_add_lynx%>%predict(complete_train_lynx)
pred_complete_train_add_lynx<-renormaliser(pred_complete_train_add_lynx,min(add_train_lynx),max(add_train_lynx))+tail(predictions_arima_lynx,-7)
mse_complete_train_add_lynx<-mean((tail(train_d_lynx,-7)-pred_complete_train_add_lynx)^2)

```

## Test data

```{r}
test<-test_d_lynx -pred_lynx$pred
test_add_no<-c(normalized_add_train_lynx[94:100,1],min_max_scale(test))
test_dataset <- timeseries_dataset_from_array(
  data = min_max_scale(test_add_no),
  targets = min_max_scale(test_add_no)[101:114],
  sequence_length = 7,
)
test_add_lynx<-best_model_add_lynx%>%predict(test_dataset)
test_add_lynx<-renormaliser(test_add_lynx,min(test),max(test))+pred_lynx$pred
mse_add_test_lynx<-mean((test_d_lynx-test_add_lynx)^2)
```

## IBM data

# Train our model
```{r, include=FALSE}
tensorflow::set_random_seed(123) #For the reproductibility
train <- normalized_add_train_ibm
# Define the number of folds for cross-validation
num_folds <- 10
set.seed(123)
# Create a data partition for cross-validation
index <-lapply(1:num_folds, function(i) ((i - 1) * 10 + 1):(i * 10))

create_model <- function(input_nodes,hidden_nodes) {
  set.seed(123)
  inputs <- layer_input(shape = c(input_nodes, 1))
  outputs <- inputs %>%
    layer_flatten()%>%
    layer_dense(hidden_nodes,activation ="tanh") %>%
    layer_dense(1, activation='linear')
  
  model <- keras_model(inputs, outputs)
  
  model %>% compile(
    optimizer = "rmsprop",
    loss = "mse",
    metrics="mae"
  )
  return(model)
}

best_mse<-100000
cv_results1<-numeric(num_folds)

train_model <- function(input_nodes, hidden_nodes) {
  for (t in 1:10 ){
    train_sequence <- train[-index[[t]], ]
    val_sequence <- train[index[[t]], ]
    
    train_dataset <- timeseries_dataset_from_array(
    data =  train_sequence,
    targets = tail(train_sequence,-input_nodes),
    sequence_length = input_nodes,
  )
  
    val_dataset <- timeseries_dataset_from_array(
    data = val_sequence,
    targets = tail(val_sequence,-input_nodes),
    sequence_length = input_nodes,
  )
  
    
    # Create and train the model
    model <- create_model(input_nodes, hidden_nodes)
    model %>% fit(
      train_dataset,
      epochs = 1000,
      shuffle=FALSE,
    )
  
    # Evaluate the model on the test set
    predictions <- model %>% predict(val_dataset)
    pred_final<-renormaliser(predictions,min(val_sequence), max(val_sequence)) 
    mse <- mean((pred_final - tail(val_sequence,-input_nodes))^2)
    cv_results1[t]<-mse
    if (mse<best_mse){
      best_model <- model
      best_mse<-mse}
  }
  
  return(c(MSE = mean(cv_results1),Mdl=best_model))
}

input_nodes<- 6
hidden_nodes<- 5 

results_train_ibm_add<-train_model(input_nodes,hidden_nodes)
best_model_add_ibm<-results_train_ibm_add$Mdl
mse_add_train_ibm<- results_train_ibm_add$MSE

complete_train_ibm<-timeseries_dataset_from_array(
    data =  min_max_scale(add_train_ibm),
    targets = tail(min_max_scale(add_train_ibm),-6),
    sequence_length = 6,
  )
pred_complete_train_add_ibm<-best_model_add_ibm%>%predict(complete_train_ibm)
pred_complete_train_add_ibm<-renormaliser(pred_complete_train_add_ibm,min(add_train_ibm),max(add_train_ibm))+tail(predictions_arima_ibm,-6)
mse_complete_train_add_ibm<-mean((tail(train_d_ibm,-6)-pred_complete_train_add_ibm)^2)
```

## Test data

```{r}
test<-test_d_ibm-pred_ibm$pred
test_add_no<-c(normalized_add_train_ibm[294:299,1],min_max_scale(test))
test_dataset <- timeseries_dataset_from_array(
  data = min_max_scale(test_add_no),
  targets = min_max_scale(test_add_no)[300:369],
  sequence_length = 6,
)
test_add_ibm<-best_model_add_ibm%>%predict(test_dataset)
test_add_ibm<-renormaliser(test_add_ibm,min(test),max(test))+pred_ibm$pred
mse_add_test_ibm<-mean((test_d_ibm-test_add_ibm)^2)
```

## Sunspot data

# Train our model
```{r, include=FALSE}
tensorflow::set_random_seed(123) #For the reproductibility
train <- normalized_add_train_sunspot
# Define the number of folds for cross-validation
num_folds <- 10
set.seed(123)
# Create a data partition for cross-validation
index <-lapply(1:num_folds, function(i) ((i - 1) * 10 + 1):(i * 10))

create_model <- function(input_nodes,hidden_nodes) {
  set.seed(123)
  inputs <- layer_input(shape = c(input_nodes, 1))
  outputs <- inputs %>%
    layer_flatten()%>%
    layer_dense(hidden_nodes,activation ="tanh") %>%
    layer_dense(1, activation='linear')
  
  model <- keras_model(inputs, outputs)
  
  model %>% compile(
    optimizer = "rmsprop",
    loss = "mse",
    metrics="mae"
  )
  return(model)
}

best_mse<-100000
cv_results1<-numeric(num_folds)

train_model <- function(input_nodes, hidden_nodes) {
  for (t in 1:10 ){
    train_sequence <- train[-index[[t]], ]
    val_sequence <- train[index[[t]], ]
    
    train_dataset <- timeseries_dataset_from_array(
    data =  train_sequence,
    targets = tail(train_sequence,-input_nodes),
    sequence_length = input_nodes,
  )
  
    val_dataset <- timeseries_dataset_from_array(
    data = val_sequence,
    targets = tail(val_sequence,-input_nodes),
    sequence_length = input_nodes,
  )
  
    
    # Create and train the model
    model <- create_model(input_nodes, hidden_nodes)
    model %>% fit(
      train_dataset,
      epochs = 1000,
      shuffle=FALSE,
    )
  
    # Evaluate the model on the test set
    predictions <- model %>% evaluate(val_dataset)
    pred_final<-renormaliser(predictions,min(val_sequence), max(val_sequence)) 
    mse <- mean((pred_final - tail(val_sequence,-input_nodes))^2)
    cv_results1[t]<-mse
    if (mse<best_mse){
      best_model <- model}
  }
  
  return(c(MSE = mean(cv_results1),Mdl=best_model))
}

input_nodes<- 4
hidden_nodes<- 4

results_train_sunspot_add<-train_model(input_nodes,hidden_nodes)
best_model_add_sunspot<-results_train_sunspot_add$Mdl
mse_add_train_sunspot<- results_train_sunspot_add$MSE

complete_train_sunspot<-timeseries_dataset_from_array(
    data =  min_max_scale(add_train_sunspot),
    targets = tail(min_max_scale(add_train_sunspot),-4),
    sequence_length = 4,
  )
pred_complete_train_add_sunspot<-best_model_add_sunspot%>%predict(complete_train_sunspot)
pred_complete_train_add_sunspot<-renormaliser(pred_complete_train_add_sunspot,min(add_train_sunspot),max(add_train_sunspot))+tail(predictions_arima_sunspot,-4)
mse_complete_train_add_sunspot<-mean((tail(train_d_sunspot,-4)-pred_complete_train_add_sunspot)^2)
```

## Test data

```{r}
test<-test_d_sunspot-pred_sunspot$pred
test_add_no<-c(normalized_add_train_sunspot[218:221,1],min_max_scale(test))
test_dataset <- timeseries_dataset_from_array(
  data = min_max_scale(test_add_no),
  targets = min_max_scale(test_add_no)[222:288],
  sequence_length = 4,
)
test_add_sunspot<-best_model_add_sunspot%>%predict(test_dataset)
test_add_sunspot<-renormaliser(test_add_sunspot,min(test),max(test))+pred_sunspot$pred
mse_add_test_sunspot<-mean((test_d_sunspot-test_add_sunspot)^2)
```

### Multiplicative model Now 

## Lynx data 

# Train our model 

```{r,include=FALSE}
tensorflow::set_random_seed(123) #For the reproductibility
train <- normalized_multi_train_lynx
# Define the number of folds for cross-validation
num_folds <- 10
set.seed(123)
# Create a data partition for cross-validation
index <-lapply(1:num_folds, function(i) ((i - 1) * 10 + 1):(i * 10))

create_model <- function(input_nodes,hidden_nodes) {
  set.seed(123)
  inputs <- layer_input(shape = c(input_nodes, 1))
  outputs <- inputs %>%
    layer_flatten()%>%
    layer_dense(hidden_nodes,activation ="tanh") %>%
    layer_dense(1, activation='linear')
  
  model <- keras_model(inputs, outputs)
  
  model %>% compile(
    optimizer = "rmsprop",
    loss = "mse",
    metrics="mae"
  )
  return(model)
}

best_mse<-100000
cv_results1<-numeric(num_folds)

train_model <- function(input_nodes, hidden_nodes) {
  for (t in 1:10 ){
    train_sequence <- train[-index[[t]], ]
    val_sequence <- train[index[[t]], ]
    
    train_dataset <- timeseries_dataset_from_array(
    data =  train_sequence,
    targets = tail(train_sequence,-input_nodes),
    sequence_length = input_nodes,
  )
  
    val_dataset <- timeseries_dataset_from_array(
    data = val_sequence,
    targets = tail(val_sequence,-input_nodes),
    sequence_length = input_nodes,
  )
  
    
    # Create and train the model
    model <- create_model(input_nodes, hidden_nodes)
    model %>% fit(
      train_dataset,
      epochs = 1000,
      shuffle=FALSE,
    )
  
    # Evaluate the model on the test set
    predictions <- model %>% predict(val_dataset)
    pred_final<-renormaliser(predictions,min(val_sequence), max(val_sequence))
    mse <- mean((pred_final - tail(val_sequence,-input_nodes))^2)
    cv_results1[t]<-mse
    if (mse<best_mse){
      best_model <- model
      best_mse<-mse}
  }
  
  return(c(MSE = mean(cv_results1),Mdl=best_model))
}

input_nodes<- 7
hidden_nodes<- 5 

results_train_lynx_multi<-train_model(input_nodes,hidden_nodes)
best_model_multi_lynx<-results_train_lynx_multi$Mdl
mse_add_multi_lynx<- results_train_lynx_multi$MSE

complete_train_lynx<-timeseries_dataset_from_array(
    data =  min_max_scale(multi_train_lynx),
    targets = tail(min_max_scale(multi_train_lynx),-7),
    sequence_length = 7,
  )
pred_complete_train_multi_lynx<-best_model_multi_lynx%>%predict(complete_train_lynx)
pred_complete_train_multi_lynx<-renormaliser(pred_complete_train_multi_lynx,min(multi_train_lynx),max(multi_train_lynx))*tail(predictions_arima_lynx,-7)
mse_complete_train_multi_lynx<-mean((tail(train_d_lynx,-7)-pred_complete_train_multi_lynx)^2)
```

## Test data

```{r}
test<-test_d_lynx/pred_lynx$pred
test_multi_no<-c(normalized_multi_train_lynx[94:100,1],min_max_scale(test))
test_dataset <- timeseries_dataset_from_array(
  data = min_max_scale(test_multi_no),
  targets = min_max_scale(test_multi_no)[101:114],
  sequence_length = 7,
)
test_multi_lynx<-best_model_multi_lynx%>%predict(test_dataset)
test_multi_lynx<-renormaliser(test_multi_lynx,min(test),max(test))*pred_lynx$pred
mse_multi_test_lynx<-mean((test_d_lynx-test_multi_lynx)^2)
```


## IBM data

# Train our model
```{r,include=FALSE}
tensorflow::set_random_seed(123) #For the reproductibility
train <- normalized_multi_train_ibm
# Define the number of folds for cross-validation
num_folds <- 10
set.seed(123)
# Create a data partition for cross-validation
index <-lapply(1:num_folds, function(i) ((i - 1) * 10 + 1):(i * 10))

create_model <- function(input_nodes,hidden_nodes) {
  set.seed(123)
  inputs <- layer_input(shape = c(input_nodes, 1))
  outputs <- inputs %>%
    layer_flatten()%>%
    layer_dense(hidden_nodes,activation ="tanh") %>%
    layer_dense(1, activation='linear')
  
  model <- keras_model(inputs, outputs)
  
  model %>% compile(
    optimizer = "rmsprop",
    loss = "mse",
    metrics="mae"
  )
  return(model)
}

best_mse<-100000
cv_results1<-numeric(num_folds)

train_model <- function(input_nodes, hidden_nodes) {
  for (t in 1:10 ){
    train_sequence <- train[-index[[t]], ]
    val_sequence <- train[index[[t]], ]
    
    train_dataset <- timeseries_dataset_from_array(
    data =  train_sequence,
    targets = tail(train_sequence,-input_nodes),
    sequence_length = input_nodes,
  )
  
    val_dataset <- timeseries_dataset_from_array(
    data = val_sequence,
    targets = tail(val_sequence,-input_nodes),
    sequence_length = input_nodes,
  )
  
    
    # Create and train the model
    model <- create_model(input_nodes, hidden_nodes)
    model %>% fit(
      train_dataset,
      epochs = 1000,
      shuffle=FALSE,
    )
  
    # Evaluate the model on the test set
    predictions <- model %>% predict(val_dataset)
    pred_final<-renormaliser(predictions,min(val_sequence), max(val_sequence))
    mse <- mean((pred_final - tail(val_sequence,-input_nodes))^2)
    cv_results1[t]<-mse
    if (mse<best_mse){
      best_model <- model
      best_mse<-mse}
  }
  
  return(c(MSE = mean(cv_results1),Mdl=best_model))
}

input_nodes<- 6
hidden_nodes<- 5 

results_train_ibm_multi<-train_model(input_nodes,hidden_nodes)
best_model_multi_ibm<-results_train_ibm_multi$Mdl
mse_multi_train_ibm<- results_train_ibm_multi$MSE

complete_train_ibm<-timeseries_dataset_from_array(
    data =  min_max_scale(multi_train_ibm),
    targets = tail(min_max_scale(multi_train_ibm),-6),
    sequence_length = 6,
  )
pred_complete_train_multi_ibm<-best_model_multi_ibm%>%predict(complete_train_ibm)
pred_complete_train_multi_ibm<-renormaliser(pred_complete_train_multi_ibm,min(multi_train_lynx),max(multi_train_ibm))*tail(predictions_arima_ibm,-6)
mse_complete_train_multi_ibm<-mean((tail(train_d_ibm,-6)-pred_complete_train_multi_ibm)^2)
```

## Test data

```{r}
test<-test_d_ibm/pred_ibm$pred
test_multi_no<-c(normalized_multi_train_ibm[294:299,1],min_max_scale(test))
test_dataset <- timeseries_dataset_from_array(
  data = min_max_scale(test_multi_no),
  targets = min_max_scale(test_multi_no)[300:369],
  sequence_length = 6,
)
test_multi_ibm<-best_model_multi_ibm%>%predict(test_dataset)
test_multi_ibm<-renormaliser(test_multi_ibm,min(test),max(test))*pred_ibm$pred
mse_multi_test_ibm<-mean((test_d_ibm-test_multi_ibm)^2)
```

## Sunspot data

# Train our model
```{r,include=FALSE}
tensorflow::set_random_seed(123) #For the reproductibility
train <- normalized_multi_train_sunspot
# Define the number of folds for cross-validation
num_folds <- 10
set.seed(123)
# Create a data partition for cross-validation
index <-lapply(1:num_folds, function(i) ((i - 1) * 10 + 1):(i * 10))

create_model <- function(input_nodes,hidden_nodes) {
  set.seed(123)
  inputs <- layer_input(shape = c(input_nodes, 1))
  outputs <- inputs %>%
    layer_flatten()%>%
    layer_dense(hidden_nodes,activation ="tanh") %>%
    layer_dense(1, activation='linear')
  
  model <- keras_model(inputs, outputs)
  
  model %>% compile(
    optimizer = "rmsprop",
    loss = "mse",
    metrics="mae"
  )
  return(model)
}

best_mse<-100000
cv_results1<-numeric(num_folds)

train_model <- function(input_nodes, hidden_nodes) {
  for (t in 1:10 ){
    train_sequence <- train[-index[[t]], ]
    val_sequence <- train[index[[t]], ]
    
    train_dataset <- timeseries_dataset_from_array(
    data =  min_max_scale(train_sequence),
    targets = tail(train_sequence,-input_nodes),
    sequence_length = input_nodes,
  )
  
    val_dataset <- timeseries_dataset_from_array(
    data = val_sequence,
    targets = tail(val_sequence,-input_nodes),
    sequence_length = input_nodes,
  )
  
    
    # Create and train the model
    model <- create_model(input_nodes, hidden_nodes)
    model %>% fit(
      train_dataset,
      epochs = 1000,
      shuffle=FALSE,
    )
  
    # Evaluate the model on the test set
    predictions <- model %>% predict(val_dataset)
    pred_final<-renormaliser(predictions,min(val_sequence), max(val_sequence)) 
    mse <- mean((pred_final - tail(val_sequence,-input_nodes))^2)
    cv_results1[t]<-mse
    if (mse<best_mse){
      best_model <- model
      best_mse<-mse}
  }
  
  return(c(MSE = mean(cv_results1),Mdl=best_model))
}

input_nodes<- 4
hidden_nodes<- 4

results_train_sunspot_multi<-train_model(input_nodes,hidden_nodes)
best_model_multi_sunspot<-results_train_sunspot_multi$Mdl
mse_multi_train_sunspot<- results_train_sunspot_multi$MSE

complete_train_sunspot<-timeseries_dataset_from_array(
    data =  min_max_scale(multi_train_sunspot),
    targets = tail(min_max_scale(multi_train_sunspot),-4),
    sequence_length = 4,
  )
pred_complete_train_multi_sunspot<-best_model_multi_sunspot%>%predict(complete_train_sunspot)
pred_complete_train_multi_sunspot<-renormaliser(pred_complete_train_multi_sunspot,min(multi_train_sunspot),max(multi_train_sunspot))*tail(predictions_arima_sunspot,-4)
mse_complete_train_multi_sunspot<-mean((tail(train_d_sunspot,-4)-pred_complete_train_multi_sunspot)^2)
```

## Test data

```{r}
test<-test_d_sunspot/pred_sunspot$pred
test_multi_no<-c(normalized_multi_train_sunspot[218:221,1],min_max_scale(test))
test_dataset <- timeseries_dataset_from_array(
  data = min_max_scale(test_multi_no),
  targets = min_max_scale(test_multi_no)[222:288],
  sequence_length = 4,
)
test_multi_sunspot<-best_model_multi_sunspot%>%predict(test_dataset)
test_multi_sunspot<-renormaliser(test_multi_sunspot,min(test),max(test))*pred_sunspot$pred
mse_multi_test_sunspot<-mean((test_d_sunspot-test_multi_sunspot)^2)
```

## 3. Plot graph between predictions and compare mse 

## Lynx data 

# Train data

```{r}

cat("MSE test ARIMA :",mse_arima_train_lynx,"\n")
cat("MSE test ANN :",mse_complete_train_ann_lynx,"\n")
cat("MSE test ADD :",mse_complete_train_add_lynx,"\n")
cat("MSE test MULTI :",mse_complete_train_multi_lynx,"\n")


```
# Test data 

```{r}

cat("MSE test ARIMA :",mse_arima_test_lynx,"\n")
cat("MSE test ANN :",mse_ann_test_lynx,"\n")
cat("MSE test ADD :",mse_add_test_lynx,"\n")
cat("MSE test MULTI :",mse_multi_test_lynx,"\n")



```

## IBM data 

# Train data

```{r}

cat("MSE test ARIMA :",mse_arima_train_ibm,"\n")
cat("MSE test ANN :",mse_complete_train_ann_ibm,"\n")
cat("MSE test ADD :",mse_complete_train_add_ibm,"\n")
cat("MSE test MULTI :",mse_complete_train_multi_ibm,"\n")


```
# Test data 

```{r}

cat("MSE test ARIMA :",mse_arima_test_ibm,"\n")
cat("MSE test ANN :",mse_ann_test_ibm,"\n")
cat("MSE test ADD :",mse_add_test_ibm,"\n")
cat("MSE test MULTI :",mse_multi_test_ibm,"\n")



```

## Sunspot data 

# Train data

```{r}

cat("MSE test ARIMA :",mse_arima_train_sunspot,"\n")
cat("MSE test ANN :",mse_complete_train_ann_sunspot,"\n")
cat("MSE test ADD :",mse_complete_train_add_sunspot,"\n")
cat("MSE test MULTI :",mse_complete_train_multi_sunspot,"\n")


```
# Test data 

```{r}

cat("MSE test ARIMA :",mse_arima_test_sunspot,"\n")
cat("MSE test ANN :",mse_ann_test_sunspot,"\n")
cat("MSE test ADD :",mse_add_test_sunspot,"\n")
cat("MSE test MULTI :",mse_multi_test_sunspot,"\n")



```