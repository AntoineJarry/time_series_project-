---
title: "Projet_recherche"
output: html_document
date: "2024-03-15"
---

# Installing the necessary package 
```{r, include=FALSE}
set.seed(123)
library(forecast)
library(caret)
library(tfruns)
library(tensorflow)
library(keras)
library(tfdatasets)
library(fma)
tensorflow::install_tensorflow()
tensorflow::set_random_seed(123) #For the reproductibility of our ANN 
```

# Upload the same data as the paper 
```{r}

data_lynx<-log10(lynx)
data(ibmclose)
data_ibm<-ibmclose
data_sunspot<-sunspot.year[1:288]

plot.ts(data_lynx)
plot.ts(data_ibm)
plot.ts(data_sunspot)

```

# Split into train_data and test_data
```{r}
train_d_lynx<-data_lynx[1:100]
test_d_lynx<-data_lynx[101:114]

train_d_ibm<-data_ibm[1:299]
test_d_ibm<-data_ibm[300:369]

train_d_sunspot<-data_sunspot[1:221]
test_d_sunspot<-data_sunspot[222:288]

```



## Start with ARIMA models

```{r}
### ARIMA Lynx
# Train_data
ar_lynx<-arima(train_d_lynx,order=c(11,0,0))
summary(ar_lynx)
predictions_arima_lynx<-fitted(ar_lynx)
mse_arima_train_lynx<-mean((predictions_arima_lynx[8:100]-train_d_lynx[8:100])^2) #For the comparaison with ANN models who take 7 inputs
plot.ts(train_d_lynx)
lines(predictions_arima_lynx,lty=2,col='red')


# Test_data One step

train<-train_d_lynx
test<-test_d_lynx
ar<-ar_lynx
forecasts_lynx <- numeric(length(test))

for (i in 1:length(test)) {
  # Forecast one step ahead
  forecast_result <- forecast(ar, h=1)
  # Extract the forecasted value for the next time step
  forecasts_lynx[i] <- forecast_result$mean[1]
  # Update the model with the actual value from the test data
  train<-tail(train,-1)
  
  ar <- arima(c(train, test[1:i]), order=c(11,0,0))
}

mse_arima_test_lynx_1<-mean((forecasts_lynx-test_d_lynx)^2)
plot.ts(test_d_lynx,ylim = range(c(test_d_lynx, data.frame(forecasts_lynx))))
lines(data.frame(forecasts_lynx),lty=2,col='red',)

# Test_data multi-step
pred_lynx<-predict(ar_lynx,n.ahead=length(test_d_lynx))
mse_arima_test_lynx<-mean((test_d_lynx-pred_lynx$pred)^2)
plot.ts(test_d_lynx,ylim = range(c(test_d_lynx, data.frame(pred_lynx$pred))))
lines(data.frame(pred_lynx$pred),lty=2,col='red',)

```

```{r}
### ARIMA IBM
# Train_data

ar_ibm<-arima(train_d_ibm,order=c(1,1,2))
predictions_arima_ibm<-fitted(ar_ibm)
mse_arima_train_ibm<-mean((predictions_arima_ibm[7:299]-train_d_ibm[7:299])^2) #For the comparaison with ANN models who take 6 inputs
plot.ts(train_d_ibm)
lines(predictions_arima_ibm,lty=2,col='red')

# Test_data One step

train<-train_d_ibm
test<-test_d_ibm
ar<-ar_ibm
forecasts_ibm <- numeric(length(test))

for (i in 1:length(test)) {
  # Forecast one step ahead
  forecast_result <- forecast(ar, h=1)
  # Extract the forecasted value for the next time step
  forecasts_ibm[i] <- forecast_result$mean[1]
  # Update the model with the actual value from the test data
  train<-tail(train,-1)
  
  ar <- arima(c(train, test[1:i]), order=c(1,1,2))
}

mse_arima_test_ibm_1<-mean((forecasts_ibm-test_d_ibm)^2)
plot.ts(test_d_ibm,ylim = range(c(test_d_ibm, data.frame(forecasts_ibm))))
lines(data.frame(forecasts_ibm),lty=2,col='red',)

# Test_data
pred_ibm<-predict(ar_ibm,n.ahead=length(test_d_ibm))
mse_arima_test_ibm<-mean((test_d_ibm-pred_ibm$pred)^2)
plot.ts(test_d_ibm,ylim = range(c(test_d_ibm, data.frame(pred_ibm$pred))))
lines(data.frame(pred_ibm$pred),lty=2,col='red')
```

```{r}
### ARIMA Sunspot
# Train_data
ar_sunspot<-arima(train_d_sunspot,order=c(9,0,0))
predictions_arima_sunspot<-fitted(ar_sunspot)
mse_arima_train_sunspot<-mean((predictions_arima_sunspot[5:100]-train_d_sunspot[5:100])^2) #For the comparaison with ANN models who take 4 inputs
plot.ts(train_d_sunspot)
lines(predictions_arima_sunspot,lty=2,col='red')

# Test_data One step

train<-train_d_sunspot
test<-test_d_sunspot
ar<-ar_sunspot
forecasts_sunspot <- numeric(length(test))

for (i in 1:length(test)) {
  # Forecast one step ahead
  forecast_result <- forecast(ar, h=1)
  # Extract the forecasted value for the next time step
  forecasts_sunspot[i] <- forecast_result$mean[1]
  # Update the model with the actual value from the test data
  train<-tail(train,-1)
  
  ar <- arima(c(train, test[1:i]), order=c(9,0,0))
}

mse_arima_test_sunspot_1<-mean((forecasts_sunspot-test_d_sunspot)^2)
plot.ts(test_d_sunspot,ylim = range(c(test_d_sunspot, data.frame(forecasts_sunspot))))
lines(data.frame(forecasts_sunspot),lty=2,col='red',)

# Test_data
pred_sunspot<-predict(ar_sunspot,n.ahead=length(test_d_sunspot))
mse_arima_test_sunspot<-mean((test_d_sunspot-pred_sunspot$pred)^2)
plot.ts(test_d_sunspot,ylim = range(c(test_d_sunspot, data.frame(pred_sunspot$pred))))
lines(data.frame(pred_sunspot$pred),lty=2,col='red',)
```

### Now ANN model alone

Make function to normalize and renormalize for the ann models
```{r, include=FALSE}
# We need to normalize our data before use them in ANN 

min_max_scale <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

renormaliser <- function(x, min_val, max_val) {
  denormalized <- x * (max_val - min_val) + min_val
  return(denormalized)
}

```

## Lynx data

# Train our model
```{r,include=FALSE}
tensorflow::set_random_seed(123) #For the reproductibility
train <- data.frame(train_d_lynx)
# Define the number of folds for cross-validation
num_folds <- 10
set.seed(123)
# Create a data partition for cross-validation
index <-lapply(1:num_folds, function(i) ((i - 1) * 10 + 1):(i * 10))

create_model <- function(input_nodes,hidden_nodes) {
  set.seed(123)
  inputs <- layer_input(shape = c(input_nodes, 1))
  outputs <- inputs %>%
    layer_flatten()%>%
    layer_dense(hidden_nodes,activation ="tanh") %>%
    layer_dense(1, activation='linear')
  
  model <- keras_model(inputs, outputs)
  
  model %>% compile(
    optimizer = "rmsprop",
    loss = "mse",
    metrics="mae"
  )
  return(model)
}

best_mse<-100000
cv_results1<-numeric(num_folds)

train_model <- function(input_nodes, hidden_nodes) {
  for (t in 1:10 ){
    train_sequence <- train[-index[[t]], ]
    val_sequence <- train[index[[t]], ]
    
    train_dataset <- timeseries_dataset_from_array(
    data =  min_max_scale(train_sequence),
    targets = tail(min_max_scale(train_sequence),-input_nodes),
    sequence_length = input_nodes,
  )
  
    val_dataset <- timeseries_dataset_from_array(
    data = min_max_scale(val_sequence),
    targets = tail(min_max_scale(val_sequence),-input_nodes),
    sequence_length = input_nodes,
  )
  
    
    # Create and train the model
    model <- create_model(input_nodes, hidden_nodes)
    model %>% fit(
      train_dataset,
      epochs = 1000,
      shuffle=FALSE,
    )
    
  
    # Evaluate the model on the test set
    predictions <- model %>% predict(val_dataset)
    pred_final<-renormaliser(predictions,min(val_sequence), max(val_sequence))
    mse <- mean((pred_final - tail(val_sequence,-input_nodes))^2)
    cv_results1[t]<-mse
    if (mse<best_mse){
      best_model <- model
      best_mse<-mse}
  }
  
  return(c(MSE = mean(cv_results1),Mdl=best_model))
}

input_nodes<- 7
hidden_nodes<- 5 

results_train_lynx_ann<-train_model(input_nodes,hidden_nodes)
best_model_ann_lynx<-results_train_lynx_ann$Mdl
mse_ann_train_lynx<- results_train_lynx_ann$MSE

complete_train_lynx<-timeseries_dataset_from_array(
    data =  min_max_scale(train_d_lynx),
    targets = tail(min_max_scale(train_d_lynx),-7),
    sequence_length = 7,
  )
pred_complete_train_ann_lynx<-best_model_ann_lynx%>%predict(complete_train_lynx)
pred_complete_train_ann_lynx<-renormaliser(pred_complete_train_ann_lynx,min(train),max(train))
mse_complete_train_ann_lynx<-mean((tail(train_d_lynx,-7)-pred_complete_train_ann_lynx)^2)
```

## Test data

```{r}
test<-data_lynx[94:114]
test_dataset <- timeseries_dataset_from_array(
  data = min_max_scale(test),
  targets = min_max_scale(test)[101:114],
  sequence_length = 7,
)
test_ann_lynx<-best_model_ann_lynx%>%predict(test_dataset)
test_ann_lynx<-renormaliser(test_ann_lynx,min(test),max(test))
mse_ann_test_lynx<-mean((test_d_lynx-test_ann_lynx)^2)

plot.ts(test_d_lynx,ylim = range(c(test_d_lynx, data.frame(test_ann_lynx))))
lines(data.frame(test_ann_lynx),lty=2,col='red',)
```

## IBM data

# Train our model
```{r,include=FALSE}
tensorflow::set_random_seed(123) #For the reproductibility
train <- data.frame(train_d_ibm)
# Define the number of folds for cross-validation
num_folds <- 10
set.seed(123)
# Create a data partition for cross-validation
index <-lapply(1:num_folds, function(i) ((i - 1) * 10 + 1):(i * 10))

create_model <- function(input_nodes,hidden_nodes) {
  set.seed(123)
  inputs <- layer_input(shape = c(input_nodes, 1))
  outputs <- inputs %>%
    layer_flatten()%>%
    layer_dense(hidden_nodes,activation ="tanh") %>%
    layer_dense(1, activation='linear')
  
  model <- keras_model(inputs, outputs)
  
  model %>% compile(
    optimizer = "rmsprop",
    loss = "mse",
    metrics="mae"
  )
  return(model)
}

best_mse<-100000
cv_results1<-numeric(num_folds)

train_model <- function(input_nodes, hidden_nodes) {
  for (t in 1:10 ){
    train_sequence <- train[-index[[t]], ]
    val_sequence <- train[index[[t]], ]
    
    train_dataset <- timeseries_dataset_from_array(
    data =  min_max_scale(train_sequence),
    targets = tail(min_max_scale(train_sequence),-input_nodes),
    sequence_length = input_nodes,
  )
  
    val_dataset <- timeseries_dataset_from_array(
    data = min_max_scale(val_sequence),
    targets = tail(min_max_scale(val_sequence),-input_nodes),
    sequence_length = input_nodes,
  )
  
    
    # Create and train the model
    model <- create_model(input_nodes, hidden_nodes)
    model %>% fit(
      train_dataset,
      epochs = 1000,
      shuffle=FALSE,
    )
  
    # Evaluate the model on the test set
    predictions <- model %>% predict(val_dataset)
    pred_final<-renormaliser(predictions,min(val_sequence), max(val_sequence))
    mse <- mean((pred_final - tail(val_sequence,-input_nodes))^2)
    cv_results1[t]<-mse
    if (mse<best_mse){
      best_model <- model
      best_mse<-mse}
  }
  
  return(c(MSE = mean(cv_results1),Mdl=best_model))
}

input_nodes<- 6
hidden_nodes<- 5 

results_train_ibm_ann<-train_model(input_nodes,hidden_nodes)
best_model_ann_ibm<-results_train_ibm_ann$Mdl
mse_ann_train_ibm<- results_train_ibm_ann$MSE

complete_train_ibm<-timeseries_dataset_from_array(
    data =  min_max_scale(train_d_ibm),
    targets = tail(min_max_scale(train_d_ibm),-6),
    sequence_length = 6,
  )
pred_complete_train_ann_ibm<-best_model_ann_ibm%>%predict(complete_train_ibm)
pred_complete_train_ann_ibm<-renormaliser(pred_complete_train_ann_ibm,min(train),max(train))
mse_complete_train_ann_ibm<-mean((tail(train_d_ibm,-6)-pred_complete_train_ann_ibm)^2)
```

## Test data

```{r}
test<-data_ibm[294:369]
test_dataset <- timeseries_dataset_from_array(
  data = min_max_scale(test),
  targets = min_max_scale(test)[300:369],
  sequence_length = 6,
)
test_ann_ibm<-best_model_ann_ibm%>%predict(test_dataset)
test_ann_ibm<-renormaliser(test_ann_ibm,min(test),max(test))
mse_ann_test_ibm<-mean((test_d_ibm-test_ann_ibm)^2)

plot.ts(test_d_ibm,ylim = range(c(test_d_ibm, data.frame(test_ann_ibm))))
lines(data.frame(test_ann_ibm),lty=2,col='red',)
```

## Sunspot data

# Train our model
```{r,include=FALSE}
tensorflow::set_random_seed(123) #For the reproductibility
train <- data.frame(train_d_sunspot)
# Define the number of folds for cross-validation
num_folds <- 10
set.seed(123)
# Create a data partition for cross-validation
index <-lapply(1:num_folds, function(i) ((i - 1) * 10 + 1):(i * 10))

create_model <- function(input_nodes,hidden_nodes) {
  set.seed(123)
  inputs <- layer_input(shape = c(input_nodes, 1))
  outputs <- inputs %>%
    layer_flatten()%>%
    layer_dense(hidden_nodes,activation ="tanh") %>%
    layer_dense(1, activation='linear')
  
  model <- keras_model(inputs, outputs)
  
  model %>% compile(
    optimizer = "rmsprop",
    loss = "mse",
    metrics="mae"
  )
  return(model)
}

best_mse<-100000
cv_results1<-numeric(num_folds)

train_model <- function(input_nodes, hidden_nodes) {
  for (t in 1:10 ){
    train_sequence <- train[-index[[t]], ]
    val_sequence <- train[index[[t]], ]
    
    train_dataset <- timeseries_dataset_from_array(
    data =  min_max_scale(train_sequence),
    targets = tail(min_max_scale(train_sequence),-input_nodes),
    sequence_length = input_nodes,
  )
  
    val_dataset <- timeseries_dataset_from_array(
    data = min_max_scale(val_sequence),
    targets = tail(min_max_scale(val_sequence),-input_nodes),
    sequence_length = input_nodes,
  )
  
    
    # Create and train the model
    model <- create_model(input_nodes, hidden_nodes)
    model %>% fit(
      train_dataset,
      epochs = 1000,
      shuffle=FALSE,
    )
  
    # Evaluate the model on the test set
    predictions <- model %>% predict(val_dataset)
    pred_final<-renormaliser(predictions,min(val_sequence), max(val_sequence))
    mse <- mean((pred_final - tail(val_sequence,-input_nodes))^2)
    cv_results1[t]<-mse
    if (mse<best_mse){
      best_model <- model
      best_mse<-mse}
  }
  
  return(c(MSE = mean(cv_results1),Mdl=best_model))
}

input_nodes<- 4
hidden_nodes<- 4

results_train_sunspot_ann<-train_model(input_nodes,hidden_nodes)
best_model_ann_sunspot<-results_train_sunspot_ann$Mdl
mse_ann_ann_sunspot<- results_train_sunspot_ann$MSE

complete_train_sunspot<-timeseries_dataset_from_array(
    data =  min_max_scale(train_d_sunspot),
    targets = tail(min_max_scale(train_d_sunspot),-4),
    sequence_length = 4,
  )
pred_complete_train_ann_sunspot<-best_model_ann_sunspot%>%predict(complete_train_sunspot)
pred_complete_train_ann_sunspot<-renormaliser(pred_complete_train_ann_sunspot,min(train),max(train))
mse_complete_train_ann_sunspot<-mean((tail(train_d_sunspot,-4)-pred_complete_train_ann_sunspot)^2)
```

## Test data

```{r}
test<-data_sunspot[218:288]
test_dataset <- timeseries_dataset_from_array(
  data = min_max_scale(test),
  targets = min_max_scale(test)[222:288],
  sequence_length = 4,
)
test_ann_sunspot<-best_model_ann_sunspot%>%predict(test_dataset)
test_ann_sunspot<-renormaliser(test_ann_sunspot,min(test),max(test))
mse_ann_test_sunspot<-mean((test_d_sunspot-test_ann_sunspot)^2)

plot.ts(test_d_sunspot,ylim = range(c(test_d_sunspot, data.frame(test_ann_sunspot))))
lines(data.frame(test_ann_sunspot),lty=2,col='red',)
```

#### Second step : create the ANN-ARIMA model 

We have two type of model, the additive model and multiplicative model.
We have already use our arima model so now we can create our ANN model for our ANN-ARIMA model
```{r}
# Lynx data

add_train_lynx = train_d_lynx-predictions_arima_lynx
multi_train_lynx= train_d_lynx/predictions_arima_lynx

# IBM data 

add_train_ibm = train_d_ibm-predictions_arima_ibm
multi_train_ibm= train_d_ibm/predictions_arima_ibm

# Sunspot data 

add_train_sunspot = train_d_sunspot-predictions_arima_sunspot
multi_train_sunspot= train_d_sunspot/predictions_arima_sunspot

```

We need to normalize our data before use them in ANN 

```{r, include=FALSE}

# Lynx data 

normalized_add_train_lynx <- data.frame(min_max_scale(add_train_lynx))
normalized_multi_train_lynx <- data.frame(min_max_scale(multi_train_lynx))

# IBM data 

normalized_add_train_ibm <- data.frame(min_max_scale(add_train_ibm))
normalized_multi_train_ibm <- data.frame(min_max_scale(multi_train_ibm))

# Sunspot data 

normalized_add_train_sunspot <- data.frame(min_max_scale(add_train_sunspot))
normalized_multi_train_sunspot <- data.frame(min_max_scale(multi_train_sunspot))

```

### Additive model first 

## Lynx data 

# Train our model 

```{r,include=FALSE}
tensorflow::set_random_seed(123) #For the reproductibility
train <- normalized_add_train_lynx
# Define the number of folds for cross-validation
num_folds <- 10
set.seed(123)
# Create a data partition for cross-validation
index <-lapply(1:num_folds, function(i) ((i - 1) * 10 + 1):(i * 10))

create_model <- function(input_nodes,hidden_nodes) {
  set.seed(123)
  inputs <- layer_input(shape = c(input_nodes, 1))
  outputs <- inputs %>%
    layer_flatten()%>%
    layer_dense(hidden_nodes,activation ="tanh") %>%
    layer_dense(1, activation='linear')
  
  model <- keras_model(inputs, outputs)
  
  model %>% compile(
    optimizer = "rmsprop",
    loss = "mse",
    metrics="mae"
  )
  return(model)
}

best_mse<-100000
cv_results1<-numeric(num_folds)

train_model <- function(input_nodes, hidden_nodes) {
  for (t in 1:10 ){
    train_sequence <- train[-index[[t]], ]
    val_sequence <- train[index[[t]], ]
    
    train_dataset <- timeseries_dataset_from_array(
    data =  train_sequence,
    targets = tail(train_sequence,-input_nodes),
    sequence_length = input_nodes,
  )
  
    val_dataset <- timeseries_dataset_from_array(
    data = val_sequence,
    targets = tail(val_sequence,-input_nodes),
    sequence_length = input_nodes,
  )
  
    
    # Create and train the model
    model <- create_model(input_nodes, hidden_nodes)
    model %>% fit(
      train_dataset,
      epochs = 1000,
      shuffle=FALSE,
    )
  
    # Evaluate the model on the test set
    predictions <- model %>% predict(val_dataset)
    mse <- mean((predictions - tail(val_sequence,-input_nodes))^2)
    cv_results1[t]<-mse
    if (mse<best_mse){
      best_model <- model
      best_mse<-mse}
  }
  
  return(c(MSE = mean(cv_results1),Mdl=best_model))
}

input_nodes<- 7
hidden_nodes<- 5 

results_train_lynx_add<-train_model(input_nodes,hidden_nodes)
best_model_add_lynx<-results_train_lynx_add$Mdl
mse_add_train_lynx<- results_train_lynx_add$MSE

complete_train_lynx<-timeseries_dataset_from_array(
    data =  min_max_scale(add_train_lynx),
    targets = tail(min_max_scale(add_train_lynx),-7),
    sequence_length = 7,
  )
pred_complete_train_add_lynx<-best_model_add_lynx%>%predict(complete_train_lynx)
pred_complete_train_add_lynx<-renormaliser(pred_complete_train_add_lynx,min(add_train_lynx),max(add_train_lynx))+tail(predictions_arima_lynx,-7)
mse_complete_train_add_lynx<-mean((tail(train_d_lynx,-7)-pred_complete_train_add_lynx)^2)

```

## Test data

```{r}
test<-test_d_lynx-forecasts_lynx
test_add_no<-c(normalized_add_train_lynx[94:100,1],min_max_scale(test))
test_dataset <- timeseries_dataset_from_array(
  data = test_add_no,
  targets = test_add_no[101:114],
  sequence_length = 7,
)
test_add_lynx<-best_model_add_lynx%>%predict(test_dataset)
test_add_lynx<-renormaliser(test_add_lynx,min(test),max(test))+forecasts_lynx
mse_add_test_lynx<-mean((test_d_lynx-test_add_lynx)^2)

plot.ts(test_d_lynx,ylim = range(c(test_d_lynx, data.frame(test_add_lynx))))
lines(data.frame(test_add_lynx),lty=2,col='red',)
```

## IBM data

# Train our model
```{r, include=FALSE}
tensorflow::set_random_seed(123) #For the reproductibility
train <- normalized_add_train_ibm
# Define the number of folds for cross-validation
num_folds <- 10
set.seed(123)
# Create a data partition for cross-validation
index <-lapply(1:num_folds, function(i) ((i - 1) * 10 + 1):(i * 10))

create_model <- function(input_nodes,hidden_nodes) {
  set.seed(123)
  inputs <- layer_input(shape = c(input_nodes, 1))
  outputs <- inputs %>%
    layer_flatten()%>%
    layer_dense(hidden_nodes,activation ="tanh") %>%
    layer_dense(1, activation='linear')
  
  model <- keras_model(inputs, outputs)
  
  model %>% compile(
    optimizer = "rmsprop",
    loss = "mse",
    metrics="mae"
  )
  return(model)
}

best_mse<-100000
cv_results1<-numeric(num_folds)

train_model <- function(input_nodes, hidden_nodes) {
  for (t in 1:10 ){
    train_sequence <- train[-index[[t]], ]
    val_sequence <- train[index[[t]], ]
    
    train_dataset <- timeseries_dataset_from_array(
    data =  train_sequence,
    targets = tail(train_sequence,-input_nodes),
    sequence_length = input_nodes,
  )
  
    val_dataset <- timeseries_dataset_from_array(
    data = val_sequence,
    targets = tail(val_sequence,-input_nodes),
    sequence_length = input_nodes,
  )
  
    
    # Create and train the model
    model <- create_model(input_nodes, hidden_nodes)
    model %>% fit(
      train_dataset,
      epochs = 1000,
      shuffle=FALSE,
    )
  
    # Evaluate the model on the test set
    predictions <- model %>% predict(val_dataset)
    mse <- mean((predictions - tail(val_sequence,-input_nodes))^2)
    cv_results1[t]<-mse
    if (mse<best_mse){
      best_model <- model
      best_mse<-mse}
  }
  
  return(c(MSE = mean(cv_results1),Mdl=best_model))
}

input_nodes<- 6
hidden_nodes<- 5 

results_train_ibm_add<-train_model(input_nodes,hidden_nodes)
best_model_add_ibm<-results_train_ibm_add$Mdl
mse_add_train_ibm<- results_train_ibm_add$MSE

complete_train_ibm<-timeseries_dataset_from_array(
    data =  min_max_scale(add_train_ibm),
    targets = tail(min_max_scale(add_train_ibm),-6),
    sequence_length = 6,
  )
pred_complete_train_add_ibm<-best_model_add_ibm%>%predict(complete_train_ibm)
pred_complete_train_add_ibm<-renormaliser(pred_complete_train_add_ibm,min(add_train_ibm),max(add_train_ibm))+tail(predictions_arima_ibm,-6)
mse_complete_train_add_ibm<-mean((tail(train_d_ibm,-6)-pred_complete_train_add_ibm)^2)
```

## Test data

```{r}
test<-test_d_ibm-forecasts_ibm
test_add_no<-c(normalized_add_train_ibm[294:299,1],min_max_scale(test))
test_dataset <- timeseries_dataset_from_array(
  data = test_add_no,
  targets = test_add_no[300:369],
  sequence_length = 6,
)
test_add_ibm<-best_model_add_ibm%>%predict(test_dataset)
test_add_ibm<-renormaliser(test_add_ibm,min(test),max(test))+forecasts_ibm
mse_add_test_ibm<-mean((test_d_ibm-test_add_ibm)^2)

plot.ts(test_d_ibm,ylim = range(c(test_d_ibm, data.frame(test_add_ibm))))
lines(data.frame(test_add_ibm),lty=2,col='red',)
```

## Sunspot data

# Train our model
```{r, include=FALSE}
tensorflow::set_random_seed(123) #For the reproductibility
train <- normalized_add_train_sunspot
# Define the number of folds for cross-validation
num_folds <- 10
set.seed(123)
# Create a data partition for cross-validation
index <-lapply(1:num_folds, function(i) ((i - 1) * 10 + 1):(i * 10))

create_model <- function(input_nodes,hidden_nodes) {
  set.seed(123)
  inputs <- layer_input(shape = c(input_nodes, 1))
  outputs <- inputs %>%
    layer_flatten()%>%
    layer_dense(hidden_nodes,activation ="tanh") %>%
    layer_dense(1, activation='linear')
  
  model <- keras_model(inputs, outputs)
  
  model %>% compile(
    optimizer = "rmsprop",
    loss = "mse",
    metrics="mae"
  )
  return(model)
}

best_mse<-100000
cv_results1<-numeric(num_folds)

train_model <- function(input_nodes, hidden_nodes) {
  for (t in 1:10 ){
    train_sequence <- train[-index[[t]], ]
    val_sequence <- train[index[[t]], ]
    
    train_dataset <- timeseries_dataset_from_array(
    data =  train_sequence,
    targets = tail(train_sequence,-input_nodes),
    sequence_length = input_nodes,
  )
  
    val_dataset <- timeseries_dataset_from_array(
    data = val_sequence,
    targets = tail(val_sequence,-input_nodes),
    sequence_length = input_nodes,
  )
  
    
    # Create and train the model
    model <- create_model(input_nodes, hidden_nodes)
    model %>% fit(
      train_dataset,
      epochs = 1000,
      shuffle=FALSE,
    )
  
    # Evaluate the model on the test set
    predictions <- model %>% evaluate(val_dataset)
    mse <- mean((predictions - tail(val_sequence,-input_nodes))^2)
    cv_results1[t]<-mse
    if (mse<best_mse){
      best_model <- model}
  }
  
  return(c(MSE = mean(cv_results1),Mdl=best_model))
}

input_nodes<- 4
hidden_nodes<- 4

results_train_sunspot_add<-train_model(input_nodes,hidden_nodes)
best_model_add_sunspot<-results_train_sunspot_add$Mdl
mse_add_train_sunspot<- results_train_sunspot_add$MSE

complete_train_sunspot<-timeseries_dataset_from_array(
    data =  min_max_scale(add_train_sunspot),
    targets = tail(min_max_scale(add_train_sunspot),-4),
    sequence_length = 4,
  )
pred_complete_train_add_sunspot<-best_model_add_sunspot%>%predict(complete_train_sunspot)
pred_complete_train_add_sunspot<-renormaliser(pred_complete_train_add_sunspot,min(add_train_sunspot),max(add_train_sunspot))+tail(predictions_arima_sunspot,-4)
mse_complete_train_add_sunspot<-mean((tail(train_d_sunspot,-4)-pred_complete_train_add_sunspot)^2)
```

## Test data

```{r}
test<-test_d_sunspot-forecasts_sunspot
test_add_no<-c(normalized_add_train_sunspot[218:221,1],min_max_scale(test))
test_dataset <- timeseries_dataset_from_array(
  data = test_add_no,
  targets = test_add_no[222:288],
  sequence_length = 4,
)
test_add_sunspot<-best_model_add_sunspot%>%predict(test_dataset)
test_add_sunspot<-renormaliser(test_add_sunspot,min(test),max(test))+forecasts_sunspot
mse_add_test_sunspot<-mean((test_d_sunspot-test_add_sunspot)^2)

plot.ts(test_d_sunspot,ylim = range(c(test_d_sunspot, data.frame(test_add_sunspot))))
lines(data.frame(test_add_sunspot),lty=2,col='red',)
```

### Multiplicative model Now 

## Lynx data 

# Train our model 

```{r,include=FALSE}
tensorflow::set_random_seed(123) #For the reproductibility
train <- normalized_multi_train_lynx
# Define the number of folds for cross-validation
num_folds <- 10
set.seed(123)
# Create a data partition for cross-validation
index <-lapply(1:num_folds, function(i) ((i - 1) * 10 + 1):(i * 10))

create_model <- function(input_nodes,hidden_nodes) {
  set.seed(123)
  inputs <- layer_input(shape = c(input_nodes, 1))
  outputs <- inputs %>%
    layer_flatten()%>%
    layer_dense(hidden_nodes,activation ="tanh") %>%
    layer_dense(1, activation='linear')
  
  model <- keras_model(inputs, outputs)
  
  model %>% compile(
    optimizer = "rmsprop",
    loss = "mse",
    metrics="mae"
  )
  return(model)
}

best_mse<-100000
cv_results1<-numeric(num_folds)

train_model <- function(input_nodes, hidden_nodes) {
  for (t in 1:10 ){
    train_sequence <- train[-index[[t]], ]
    val_sequence <- train[index[[t]], ]
    
    train_dataset <- timeseries_dataset_from_array(
    data =  train_sequence,
    targets = tail(train_sequence,-input_nodes),
    sequence_length = input_nodes,
  )
  
    val_dataset <- timeseries_dataset_from_array(
    data = val_sequence,
    targets = tail(val_sequence,-input_nodes),
    sequence_length = input_nodes,
  )
  
    
    # Create and train the model
    model <- create_model(input_nodes, hidden_nodes)
    model %>% fit(
      train_dataset,
      epochs = 1000,
      shuffle=FALSE,
    )
  
    # Evaluate the model on the test set
    predictions <- model %>% predict(val_dataset)
    mse <- mean((predictions - tail(val_sequence,-input_nodes))^2)
    cv_results1[t]<-mse
    if (mse<best_mse){
      best_model <- model
      best_mse<-mse}
  }
  
  return(c(MSE = mean(cv_results1),Mdl=best_model))
}

input_nodes<- 7
hidden_nodes<- 5 

results_train_lynx_multi<-train_model(input_nodes,hidden_nodes)
best_model_multi_lynx<-results_train_lynx_multi$Mdl
mse_add_multi_lynx<- results_train_lynx_multi$MSE

complete_train_lynx<-timeseries_dataset_from_array(
    data =  min_max_scale(multi_train_lynx),
    targets = tail(min_max_scale(multi_train_lynx),-7),
    sequence_length = 7,
  )
pred_complete_train_multi_lynx<-best_model_multi_lynx%>%predict(complete_train_lynx)
pred_complete_train_multi_lynx<-renormaliser(pred_complete_train_multi_lynx,min(multi_train_lynx),max(multi_train_lynx))*tail(predictions_arima_lynx,-7)
mse_complete_train_multi_lynx<-mean((tail(train_d_lynx,-7)-pred_complete_train_multi_lynx)^2)

```

## Test data

```{r}
test<-test_d_lynx/forecasts_lynx
test_multi_no<-c(normalized_multi_train_lynx[94:100,1],min_max_scale(test))
test_dataset <- timeseries_dataset_from_array(
  data = test_multi_no,
  targets = test_multi_no[101:114],
  sequence_length = 7,
)
test_multi_lynx<-best_model_multi_lynx%>%predict(test_dataset)
test_multi_lynx<-renormaliser(test_multi_lynx,min(test),max(test))*forecasts_lynx
mse_multi_test_lynx<-mean((test_d_lynx-test_multi_lynx)^2)

plot.ts(test_d_lynx,ylim = range(c(test_d_lynx, data.frame(test_multi_lynx))))
lines(data.frame(test_multi_lynx),lty=2,col='red',)
```


## IBM data

# Train our model
```{r,include=FALSE}
tensorflow::set_random_seed(123) #For the reproductibility
train <- normalized_multi_train_ibm
# Define the number of folds for cross-validation
num_folds <- 10
set.seed(123)
# Create a data partition for cross-validation
index <-lapply(1:num_folds, function(i) ((i - 1) * 10 + 1):(i * 10))

create_model <- function(input_nodes,hidden_nodes) {
  set.seed(123)
  inputs <- layer_input(shape = c(input_nodes, 1))
  outputs <- inputs %>%
    layer_flatten()%>%
    layer_dense(hidden_nodes,activation ="tanh") %>%
    layer_dense(1, activation='linear')
  
  model <- keras_model(inputs, outputs)
  
  model %>% compile(
    optimizer = "rmsprop",
    loss = "mse",
    metrics="mae"
  )
  return(model)
}

best_mse<-100000
cv_results1<-numeric(num_folds)

train_model <- function(input_nodes, hidden_nodes) {
  for (t in 1:10 ){
    train_sequence <- train[-index[[t]], ]
    val_sequence <- train[index[[t]], ]
    
    train_dataset <- timeseries_dataset_from_array(
    data =  train_sequence,
    targets = tail(train_sequence,-input_nodes),
    sequence_length = input_nodes,
  )
  
    val_dataset <- timeseries_dataset_from_array(
    data = val_sequence,
    targets = tail(val_sequence,-input_nodes),
    sequence_length = input_nodes,
  )
  
    
    # Create and train the model
    model <- create_model(input_nodes, hidden_nodes)
    model %>% fit(
      train_dataset,
      epochs = 1000,
      shuffle=FALSE,
    )
  
    # Evaluate the model on the test set
    predictions <- model %>% predict(val_dataset)
    mse <- mean((predictions - tail(val_sequence,-input_nodes))^2)
    cv_results1[t]<-mse
    if (mse<best_mse){
      best_model <- model
      best_mse<-mse}
  }
  
  return(c(MSE = mean(cv_results1),Mdl=best_model))
}

input_nodes<- 6
hidden_nodes<- 5 

results_train_ibm_multi<-train_model(input_nodes,hidden_nodes)
best_model_multi_ibm<-results_train_ibm_multi$Mdl
mse_multi_train_ibm<- results_train_ibm_multi$MSE

complete_train_ibm<-timeseries_dataset_from_array(
    data =  min_max_scale(multi_train_ibm),
    targets = tail(min_max_scale(multi_train_ibm),-6),
    sequence_length = 6,
  )
pred_complete_train_multi_ibm<-best_model_multi_ibm%>%predict(complete_train_ibm)
pred_complete_train_multi_ibm<-renormaliser(pred_complete_train_multi_ibm,min(multi_train_ibm),max(multi_train_ibm))*tail(predictions_arima_ibm,-6)
mse_complete_train_multi_ibm<-mean((tail(train_d_ibm,-6)-pred_complete_train_multi_ibm)^2)

```

## Test data

```{r}
test<-test_d_ibm/forecasts_ibm
test_multi_no<-c(normalized_multi_train_ibm[294:299,1],min_max_scale(test))
test_dataset <- timeseries_dataset_from_array(
  data = test_multi_no,
  targets = test_multi_no[300:369],
  sequence_length = 6,
)
test_multi_ibm<-best_model_multi_ibm%>%predict(test_dataset)
test_multi_ibm<-renormaliser(test_multi_ibm,min(test),max(test))*forecasts_ibm
mse_multi_test_ibm<-mean((test_d_ibm-test_multi_ibm)^2)

plot.ts(test_d_ibm,ylim = range(c(test_d_ibm, data.frame(test_multi_ibm))))
lines(data.frame(test_multi_ibm),lty=2,col='red',)
```

## Sunspot data

# Train our model
```{r,include=FALSE}
tensorflow::set_random_seed(123) #For the reproductibility
train <- normalized_multi_train_sunspot
# Define the number of folds for cross-validation
num_folds <- 10
set.seed(123)
# Create a data partition for cross-validation
index <-lapply(1:num_folds, function(i) ((i - 1) * 10 + 1):(i * 10))

create_model <- function(input_nodes,hidden_nodes) {
  set.seed(123)
  inputs <- layer_input(shape = c(input_nodes, 1))
  outputs <- inputs %>%
    layer_flatten()%>%
    layer_dense(hidden_nodes,activation ="tanh") %>%
    layer_dense(1, activation='linear')
  
  model <- keras_model(inputs, outputs)
  
  model %>% compile(
    optimizer = "rmsprop",
    loss = "mse",
    metrics="mae"
  )
  return(model)
}

best_mse<-100000
cv_results1<-numeric(num_folds)

train_model <- function(input_nodes, hidden_nodes) {
  for (t in 1:10 ){
    train_sequence <- train[-index[[t]], ]
    val_sequence <- train[index[[t]], ]
    
    train_dataset <- timeseries_dataset_from_array(
    data =  min_max_scale(train_sequence),
    targets = tail(train_sequence,-input_nodes),
    sequence_length = input_nodes,
  )
  
    val_dataset <- timeseries_dataset_from_array(
    data = val_sequence,
    targets = tail(val_sequence,-input_nodes),
    sequence_length = input_nodes,
  )
  
    
    # Create and train the model
    model <- create_model(input_nodes, hidden_nodes)
    model %>% fit(
      train_dataset,
      epochs = 1000,
      shuffle=FALSE,
    )
  
    # Evaluate the model on the test set
    predictions <- model %>% predict(val_dataset)
    mse <- mean((predictions - tail(val_sequence,-input_nodes))^2)
    cv_results1[t]<-mse
    if (mse<best_mse){
      best_model <- model
      best_mse<-mse}
  }
  
  return(c(MSE = mean(cv_results1),Mdl=best_model))
}

input_nodes<- 4
hidden_nodes<- 4

results_train_sunspot_multi<-train_model(input_nodes,hidden_nodes)
best_model_multi_sunspot<-results_train_sunspot_multi$Mdl
mse_multi_train_sunspot<- results_train_sunspot_multi$MSE

complete_train_sunspot<-timeseries_dataset_from_array(
    data =  min_max_scale(multi_train_sunspot),
    targets = tail(min_max_scale(multi_train_sunspot),-4),
    sequence_length = 4,
  )
pred_complete_train_multi_sunspot<-best_model_multi_sunspot%>%predict(complete_train_sunspot)
pred_complete_train_multi_sunspot<-renormaliser(pred_complete_train_multi_sunspot,min(multi_train_sunspot),max(multi_train_sunspot))*tail(predictions_arima_sunspot,-4)
mse_complete_train_multi_sunspot<-mean((tail(train_d_sunspot,-4)-pred_complete_train_multi_sunspot)^2)
```

## Test data

```{r}
test<-test_d_sunspot/forecasts_sunspot
test_multi_no<-c(normalized_multi_train_sunspot[218:221,1],min_max_scale(test))
test_dataset <- timeseries_dataset_from_array(
  data = test_multi_no,
  targets = test_multi_no[222:288],
  sequence_length = 4,
)
test_multi_sunspot<-best_model_multi_sunspot%>%predict(test_dataset)
test_multi_sunspot<-renormaliser(test_multi_sunspot,min(test),max(test))*forecasts_sunspot
mse_multi_test_sunspot<-mean((test_d_sunspot-test_multi_sunspot)^2)
png("multi_sunspot.png")
plot.ts(test_d_sunspot,ylim = range(c(test_d_sunspot, data.frame(test_multi_sunspot))))
lines(data.frame(test_multi_sunspot),lty=2,col='red',)
dev.off()
```

## 3. Plot graph between predictions and compare mse 

## Lynx data 

# Train data

```{r}

cat("MSE train ARIMA :",mse_arima_train_lynx,"\n")
cat("MSE train ANN :",mse_complete_train_ann_lynx,"\n")
cat("MSE train ADD :",mse_complete_train_add_lynx,"\n")
cat("MSE train MULTI :",mse_complete_train_multi_lynx,"\n")


```
# Test data 

```{r}

cat("MSE test ARIMA :",mse_arima_test_lynx_1,"\n")
cat("MSE test ANN :",mse_ann_test_lynx,"\n")
cat("MSE test ADD :",mse_add_test_lynx,"\n")
cat("MSE test MULTI :",mse_multi_test_lynx,"\n")



```

## IBM data 

# Train data

```{r}

cat("MSE train ARIMA :",mse_arima_train_ibm,"\n")
cat("MSE train ANN :",mse_complete_train_ann_ibm,"\n")
cat("MSE train ADD :",mse_complete_train_add_ibm,"\n")
cat("MSE train MULTI :",mse_complete_train_multi_ibm,"\n")


```
# Test data 

```{r}

cat("MSE test ARIMA :",mse_arima_test_ibm_1,"\n")
cat("MSE test ANN :",mse_ann_test_ibm,"\n")
cat("MSE test ADD :",mse_add_test_ibm,"\n")
cat("MSE test MULTI :",mse_multi_test_ibm,"\n")



```

## Sunspot data 

# Train data

```{r}

cat("MSE train ARIMA :",mse_arima_train_sunspot,"\n")
cat("MSE train ANN :",mse_complete_train_ann_sunspot,"\n")
cat("MSE train ADD :",mse_complete_train_add_sunspot,"\n")
cat("MSE train MULTI :",mse_complete_train_multi_sunspot,"\n")


```
# Test data 

```{r}

cat("MSE test ARIMA :",mse_arima_test_sunspot_1,"\n")
cat("MSE test ANN :",mse_ann_test_sunspot,"\n")
cat("MSE test ADD :",mse_add_test_sunspot,"\n")
cat("MSE test MULTI :",mse_multi_test_sunspot,"\n")



```
```{r}
## TRAIN
# Création des données
ADD <- c(mse_complete_train_add_lynx,mse_complete_train_add_ibm,mse_complete_train_add_sunspot)
MULTI <- c(mse_complete_train_multi_lynx,54.022,197.89)
ANN <- c(mse_complete_train_ann_lynx,mse_complete_train_ann_ibm,mse_complete_train_ann_sunspot)
ARIMA<-c(mse_arima_train_lynx,mse_arima_train_ibm,mse_arima_train_sunspot)

# Création du dataframe
resultats_train <- data.frame(ARIMA=ARIMA, ANN=ANN, ADD=ADD,MULTI=MULTI)

# Définir les noms pour les lignes
rownames(resultats_train) <- c("Lynx", "IBM", "Sunspot")

# Affichage du tableau
print(resultats_train)

write.csv(resultats_train, "resultats_train.csv")
```
```{r}
## TEST
# Création des données
ADD <- c(mse_add_test_lynx,mse_add_test_ibm,mse_add_test_sunspot)
MULTI <- c(mse_multi_test_lynx,54.3615,941.82)
ANN <- c(mse_ann_test_lynx,mse_ann_test_ibm,mse_ann_test_sunspot)
ARIMA<-c(mse_arima_test_lynx_1,mse_arima_test_ibm_1,mse_arima_test_sunspot_1)

# Création du dataframe
resultats_test <- data.frame(ARIMA=ARIMA, ANN=ANN, ADD=ADD,MULTI=MULTI)

# Définir les noms pour les lignes
rownames(resultats_test) <- c("Lynx", "IBM", "Sunspot")

# Affichage du tableau
print(resultats_test)

write.csv(resultats_test, "resultats_test.csv")
```

